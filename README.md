# CANIS-HACKATHON-CREAM_TEAM

Group Members: 
Jori Duguid & Jacob Lever, 2nd year software engineering students at University of Calgary


Motivation: We wanted to explore the potential ways humans can reduce their vulnerability to misinformation they see, whether that be through awareness 
the things that signify fake information or through machine learning and artificial intelligence.


## About our Submission:

Part 1. Exploratory Data analysis: used the data provided to develop understanding of things to look out for when reading articles online, news, and any other media a person may be exposed to.
By understanding the traits of fake content, it may be able to help people stay safe.
- CASE 1. Exploring the effect of word length in content validity. See the file length_study.py for the information about the code and run the program to show the results.
- CASE 2. Exploring the frequency of emotional language in fake and real content. See emotion_study.py.
- CASE 3. Exploring the frequency of bias words in fake and real content. See bias_study.py.
- CASE 4. Exploring the frequency of clickbait in fake and real content. See clickbait_study.py.

Part 2. Machine Learning model to predict misinformation: we developed a machine learning model to train AI to detect false information, based of the data given in the offical Hackathon guidelines.
- Includes confusion matrix to test accuracy of the model.
